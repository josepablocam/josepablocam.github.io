@article{cambronero2019autogenerating,
  title={AL: autogenerating supervised learning programs},
  author={Cambronero, Jos{\'e} P and Rinard, Martin C},
  journal={Proceedings of the ACM on Programming Languages},
  volume={3},
  number={OOPSLA},
  pages={1--28},
  year={2019},
  publisher={ACM New York, NY, USA},
  url = {https://doi.org/10.1145/3360601},
}

@inproceedings{cambronero2019active,
  title={Active learning for software engineering},
  author={Cambronero, Jos{\'e} P and Dang, Thurston HY and Vasilakis, Nikos and Shen, Jiasi and Wu, Jerry and Rinard, Martin C},
  booktitle={Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
  pages={62--78},
  year={2019},
  url = {https://doi.org/10.1145/3359591.3359732},
}

@inproceedings{cambronero2019characterizing,
  title={Characterizing developer use of automatically generated patches},
  author={Cambronero, Jos{\'e} Pablo and Shen, Jiasi and Cito, J{\"u}rgen and Glassman, Elena and Rinard, Martin},
  booktitle={2019 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
  pages={181--185},
  year={2019},
  organization={IEEE},
  url = {https://www.computer.org/csdl/proceedings-article/vl-hcc/2019/08818884/1dsfTbnvEJy}
}

@article{cambronero2018incremental,
  title={Incremental Color Quantization for Color-Vision-Deficient Observers Using Mobile Gaming Data},
  author={Cambronero, Jose and Stanley-Marbell, Phillip and Rinard, Martin},
  journal={arXiv preprint arXiv:1803.08420},
  year={2018}
}

@misc{appelbaum2020development-asco,
  title={Development and validation of a pancreatic cancer prediction model from electronic health records using machine learning.},
  author={Appelbaum, Limor and Cambronero, Jose Pablo and Pollick, Karla and Silva, George and Stevens, Jennifer P and Mamon, Harvey J and Kaplan, Irving D and Rinard, Martin},
  year={2020},
  publisher={American Society of Clinical Oncology}
}

@article{cambronero2017query,
  title={Query optimization for dynamic imputation},
  author={Cambronero, Jos{\'e} and Feser, John K and Smith, Micah J and Madden, Samuel},
  journal={Proceedings of the VLDB Endowment},
  volume={10},
  number={11},
  pages={1310--1321},
  year={2017},
  publisher={VLDB Endowment},
  url = {https://www.vldb.org/pvldb/vol10/p1310-feser.pdf}
}

@inproceedings{cambronero2020ams,
  title={AMS: generating AutoML search spaces from weak specifications},
  author={Cambronero, Jos{\'e} P and Cito, J{\"u}rgen and Rinard, Martin C},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={763--774},
  year={2020},
  url = {https://dl.acm.org/doi/10.1145/3368089.3409700}
}

@inproceedings{cambronero2019deep,
  title={When deep learning met code search},
  author={Cambronero, Jose and Li, Hongyu and Kim, Seohyun and Sen, Koushik and Chandra, Satish},
  booktitle={Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={964--974},
  year={2019},
  url = {https://doi.org/10.1145/3338906.3340458}
}


@article{appelbaum2020development-ejc,
  title={Development and validation of a pancreatic cancer risk model for the general population using electronic health records: An observational study},
  author={Appelbaum, Limor and Cambronero, Jos{\'e} P and Stevens, Jennifer P and Horng, Steven and Pollick, Karla and Silva, George and Haneuse, Sebastien and Piatkowski, Gail and Benhaga, Nordine and Duey, Stacey and others},
  journal={European Journal of Cancer},
  volume={143},
  pages={19--30},
  year={2020},
  publisher={Elsevier},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0959804920312934}
}


@article{dang2021inferring,
  title={Inferring Drop-in Binary Parsers from Program Executions},
  author={Dang, Thurston HY and Cambronero, Jose P and Rinard, Martin C},
  journal={arXiv preprint arXiv:2104.09669},
  year={2021}
}

@article{zogaj2021doing,
  title={Doing more with less: characterizing dataset downsampling for AutoML},
  author={Zogaj, Fatjon and Cambronero, Jos{\'e} Pablo and Rinard, Martin C and Cito, J{\"u}rgen},
  journal={Proceedings of the VLDB Endowment},
  volume={14},
  number={11},
  pages={2059--2072},
  year={2021},
  publisher={VLDB Endowment},
  url = {https://vldb.org/pvldb/vol14/p2059-zogaj.pdf}
}

@misc{appelbaum2021development,
  title={Development of a pancreatic cancer prediction model using a multinational medical records database.},
  author={Appelbaum, Limor and Berg, Alexandra and Cambronero, Jose Pablo and Dang, Thurston Hou Yeen and Jin, Charles Chuan and Zhang, Lori and Kundrot, Steven and Palchuk, Matvey and Evans, Laura A and Kaplan, Irving D and others},
  year={2021},
  publisher={American Society of Clinical Oncology}
}

@article{samak2021searching,
  title={Searching for Replacement Classes},
  author={Samak, Malavika and Cambronero, Jose Pablo and Rinard, Martin C},
  journal={arXiv preprint arXiv:2110.05638},
  year={2021}
}

@article{10.14778/3603581.3603600,
author = {Singh, Mukul and S\'{a}nchez, Jos\'{e} Cambronero and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Raza, Mohammad and Verbruggen, Gust},
title = {Cornet: Learning Table Formatting Rules By Example},
year = {2023},
issue_date = {June 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3603581.3603600},
doi = {10.14778/3603581.3603600},
abstract = {Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present Cornet, a system that tackles the novel problem of automatically learning such formatting rules from user-provided formatted cells. Cornet takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce the task of automatically learning conditional formatting rules, we compare Cornet to a wide range of symbolic and neural baselines adapted from related domains. Our results show that Cornet accurately learns rules across varying setups. Additionally, we show that in some cases Cornet can find rules that are shorter than those written by users and can also discover rules in spreadsheets that users have manually formatted. Furthermore, we present two case studies investigating the generality of our approach by extending Cornet to related data tasks (e.g., filtering) and generalizing to conditional formatting over multiple columns.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {2632â€“2644},
numpages = {13}
}

@inproceedings{10.1609/aaai.v37i4.25642,
    author = {Joshi, Harshit and Sanchez, Jos\'{e} Cambronero and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Verbruggen, Gust},
    title = {Repair is Nearly Generation: Multilingual Program Repair with LLMs},
    year = {2023},
    isbn = {978-1-57735-880-0},
    publisher = {AAAI Press},
    url = {https://doi.org/10.1609/aaai.v37i4.25642},
    doi = {10.1609/aaai.v37i4.25642},
    abstract = {Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program - a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language- specific repair engines for three of these languages.},
    booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
    articleno = {573},
    numpages = {10},
    series = {AAAI'23/IAAI'23/EAAI'23}
}

@article{wasti2022loopstack,
  title={LoopStack: a Lightweight Tensor Algebra Compiler Stack},
  author={Wasti, Bram and Cambronero, Jos{\'e} Pablo and Steiner, Benoit and Leather, Hugh and Zlateski, Aleksandar},
  journal={arXiv preprint arXiv:2205.00618},
  year={2022}
}

@article{zhang2022repairing,
  title={Repairing Bugs in Python Assignments Using Large Language Models},
  author={Zhang, Jialu and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust},
  journal={arXiv preprint arXiv:2209.14876 (to appear OOPSLA 2024)},
  year={2022}
}

@article{joshi2023flame,
  title={FLAME: A small language model for spreadsheet formulas},
  author={Joshi, Harshit and Ebenezer, Abishai and Cambronero, Jos{\'e} and Gulwani, Sumit and Kanade, Aditya and Le, Vu and Radi{\v{c}}ek, Ivan and Verbruggen, Gust},
  journal={arXiv preprint arXiv:2301.13779 (to appear AAAI 2024)},
  year={2023}
}

@article{phung2023generating,
  title={Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models},
  author={Phung, Tung and Cambronero, Jos{\'e} and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
  journal={EDM 2023},
  year={2023},
  url = {https://arxiv.org/abs/2302.04662}
}

@article{cambronero2023flashfill++,
  title={FlashFill++: Scaling Programming by Example by Cutting to the Chase},
  author={Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Perelman, Daniel and Radhakrishna, Arjun and Simon, Clint and Tiwari, Ashish},
  journal={Proceedings of the ACM on Programming Languages},
  volume={7},
  number={POPL},
  pages={952--981},
  year={2023},
  publisher={ACM New York, NY, USA},
  url = {https://doi.org/10.1145/3571226}
}

@inproceedings{10.1145/3568812.3603476,
author = {Phung, Tung and P\u{a}durean, Victor-Alexandru and Cambronero, Jos\'{e} and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
title = {Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603476},
doi = {10.1145/3568812.3603476},
abstract = {Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies. State-of-the-art models like OpenAIâ€™s ChatGPT&nbsp;[8] and GPT-4&nbsp;[9] could enhance programming education in various roles, e.g., by acting as a personalized digital tutor for a student, a digital assistant for an educator, and a digital peer for collaborative learning&nbsp;[1, 2, 7]. In our work, we seek to comprehensively evaluate and benchmark state-of-the-art large language models for various scenarios in programming education. Recent works have evaluated several large language models in the context of programming education&nbsp;[4, 6, 10, 11, 12]. However, these works are limited for several reasons: they have typically focused on evaluating a specific model for a specific education scenario (e.g., generating explanations), or have considered models that are already outdated (e.g., OpenAIâ€™s Codex&nbsp;[3] is no longer publicly available since March 2023). Consequently, there is a lack of systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios in programming education. These scenarios are designed to capture distinct roles these models could play, namely digital tutors, assistants, and peers, as discussed above. More concretely, we consider the following six scenarios: (1) program repair, i.e., fixing a studentâ€™s buggy program; (2) hint generation, i.e., providing a natural language hint to the student to help resolve current issues; (3) grading feedback, i.e., grading a studentâ€™s program w.r.t. a given rubric; (4) peer programming, i.e., completing a partially written program or generating a sketch for the solution program; (5) task creation, i.e., generating new tasks that exercise specific types of concepts or bugs; (6) contextualized explanation, i.e., explaining specific concepts or functions in the context of a given program. Our study uses a mix of quantitative and qualitative evaluation to compare the performance of these models with the performance of human tutors. We conduct our evaluation based on 5 introductory Python programming problems with a diverse set of input/output specifications. For each of these problems, we consider 5 buggy programs based on publicly accessible submissions from geeksforgeeks.org &nbsp;[5] (see Figure&nbsp;1); these buggy programs are picked to capture different types of bugs for each problem. We will provide a detailed analysis of the data and results in a longer version of this poster. Our preliminary results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutorsâ€™ performance for several scenarios.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {41â€“42},
numpages = {2},
keywords = {introductory programming education, generative AI, large language models, ChatGPT},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{singh-etal-2023-codefusion,
    title = "{C}ode{F}usion: A Pre-trained Diffusion Model for Code Generation",
    author = "Singh, Mukul  and
      Cambronero, Jos{\'e}  and
      Gulwani, Sumit  and
      Le, Vu  and
      Negreanu, Carina  and
      Verbruggen, Gust",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.716",
    doi = "10.18653/v1/2023.emnlp-main.716",
    pages = "11697--11708",
    abstract = "Imagine a developer who can only change their last line of code{---}how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.",
}

@inproceedings{10.1145/3583780.3614863,
author = {Singh, Mukul and Cambronero, Jos\'{e} and Gulwani, Sumit and Le, Vu and Verbruggen, Gust},
title = {EmFore: Online Learning of Email Folder Classification Rules},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614863},
doi = {10.1145/3583780.3614863},
abstract = {Modern email clients support predicate-based folder assignment rules that can automatically organize emails. Unfortunately, users still need to write these rules manually. Prior machine learning approaches have framed automatically assigning email to folders as a classification task and do not produce symbolic rules. Prior inductive logic programming (ILP) approaches, which generate symbolic rules, fail to learn efficiently in the online environment needed for email management. To close this gap, we present EmFORE, an online system that learns symbolic rules for email classification from observations. Our key insights to do this successfully are: (1) learning rules over a folder abstraction that supports quickly determining candidate predicates to add or replace terms in a rule, (2) ensuring that rules remain consistent with historical assignments, (3) ranking rule updates based on existing predicate and folder name similarity, and (4) building a rule suppression model to avoid surfacing low-confidence folder predictions while keeping the rule for future use. We evaluate on two popular public email corpora and compare to 13 baselines, including state-of-the-art folder assignment systems, incremental machine learning, ILP and transformer-based approaches. We find that EmFORE performs significantly better, updates four orders of magnitude faster, and is more robust than existing methods and baselines.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2280â€“2290},
numpages = {11},
keywords = {online learning, email classification, learning by examples},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@article{10.1145/3563327,
author = {Bavishi, Rohan and Joshi, Harshit and Cambronero, Jos\'{e} and Fariha, Anna and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Tiwari, Ashish},
title = {Neurosymbolic Repair for Low-Code Formula Languages},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563327},
doi = {10.1145/3563327},
abstract = {Most users of low-code platforms, such as Excel and PowerApps, write programs in domain-specific formula languages to carry out nontrivial tasks. Often users can write most of the program they want, but introduce small mistakes that yield broken formulas. These mistakes, which can be both syntactic and semantic, are hard for low-code users to identify and fix, even though they can be resolved with just a few edits. We formalize the problem of producing such edits as the last-mile repair problem. To address this problem, we developed LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages. LaMirage takes a grammar and a set of domain-specific constraints/rules, which jointly approximate the target language, and uses these to generate a repair engine that can fix formulas in that language. To tackle the challenges of localizing errors and ranking candidate repairs, LaMirage leverages neural techniques, whereas it relies on symbolic methods to generate candidate edits. This combination allows LaMirage to find repairs that satisfy the provided grammar and constraints, and then pick the most natural repair. We compare LaMirage to state-of-the-art neural and symbolic approaches on 400 real Excel and Power Fx formulas, where LaMirage outperforms all baselines. We release these benchmarks to encourage subsequent work in low-code domains.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {164},
numpages = {30},
keywords = {Low-Code, Neurosymbolic, Program Repair}
}

@article{singh2023format5,
  title={FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language},
  author={Singh, Mukul and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Nouri, Elnaz and Raza, Mohammad and Verbruggen, Gust},
  journal={arXiv preprint arXiv:2310.17306 (to appear VLDB 2024)},
  year={2023}
}


@article{gordon2023co,
  title={Co-audit: tools to help humans double-check AI-generated content},
  author={Gordon, Andrew D and Negreanu, Carina and Cambronero, Jos{\'e} and Chakravarthy, Rasika and Drosos, Ian and Fang, Hao and Mitra, Bhaskar and Richardson, Hannah and Sarkar, Advait and Simmons, Stephanie and others},
  journal={arXiv preprint arXiv:2310.01297 (to appear PLATEAU 2024)},
  year={2023}
}

@article{singha2023tabular,
  title={Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs},
  author={Singha, Ananya and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Parnin, Chris},
  journal={arXiv preprint arXiv:2310.10358 (Table Representation Learning at NeurIPS 2023)},
  year={2023}
}