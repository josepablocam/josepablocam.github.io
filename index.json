[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a senior researcher on the PROSE team at Microsoft, where we develop state-of-the-art program synthesis technologies to make software development more accessible, productive, and fun. My current line of work focuses on using neurosymbolic techniques for applications ranging from program repair to programming by example, with a particular emphasis on the former.\nSome of our recent work at PROSE includes:\n FLAME: A small language model for spreadsheet formulas(to appear AAAI 2024) Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs (Table Representation Learning Workshop at NeurIPS 2023, best paper runner up) CODEFUSION: A Pre-trained Diffusion Model for Code Generation (EMNLP 2023) FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language (to appear VLDB 2024) EmFORE: Online Learning of Email Folder Classification Rules (CIKM 2023) Learning Table Formatting Rules By Example (VLDB 2023) Large language model-based multi-lingual program repair (AAAI 2023) FlashFill++: Scaling Programming by Example by Cutting to the Chase (POPL 2023) Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models (EDM 2023) Neurosymbolic repair for low-code formula languages (OOPSLA 2022) Repairing Bugs in Python Assignments Using Large Language Models  For an up to date list of my publications, please see my Google Scholar profile.\nIf you\u0026rsquo;re a PhD student in a related area of research (e.g. SE, PL, ML) and are interested in doing an internship with me, please reach out via email with your CV and quick blurb on your interests.\nBefore joining Microsoft, I received my PhD from MIT, under the supervision of Martin Rinard. You can find a copy of my thesis here.\nBefore my PhD, I obtained an M.S. in Computer Science (NYU) and a B.A. in Economics (University of Pennsylvania). I have had the opportunity of working (both as an intern and full-time employee) across technology and finance companies, in a variety of roles. I\u0026rsquo;m originally from San José, Costa Rica.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://josecambronero.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a senior researcher on the PROSE team at Microsoft, where we develop state-of-the-art program synthesis technologies to make software development more accessible, productive, and fun. My current line of work focuses on using neurosymbolic techniques for applications ranging from program repair to programming by example, with a particular emphasis on the former.\nSome of our recent work at PROSE includes:\n FLAME: A small language model for spreadsheet formulas(to appear AAAI 2024) Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs (Table Representation Learning Workshop at NeurIPS 2023, best paper runner up) CODEFUSION: A Pre-trained Diffusion Model for Code Generation (EMNLP 2023) FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language (to appear VLDB 2024) EmFORE: Online Learning of Email Folder Classification Rules (CIKM 2023) Learning Table Formatting Rules By Example (VLDB 2023) Large language model-based multi-lingual program repair (AAAI 2023) FlashFill++: Scaling Programming by Example by Cutting to the Chase (POPL 2023) Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models (EDM 2023) Neurosymbolic repair for low-code formula languages (OOPSLA 2022) Repairing Bugs in Python Assignments Using Large Language Models  For an up to date list of my publications, please see my Google Scholar profile.","tags":null,"title":"José Cambronero","type":"authors"},{"authors":null,"categories":null,"content":"","date":1504051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504051200,"objectID":"97261f34851a44239f7308282fec3822","permalink":"https://josecambronero.com/talk/vldb-2017/","publishdate":"2017-08-30T00:00:00Z","relpermalink":"/talk/vldb-2017/","section":"talk","summary":"Unfortunately I don't have a video of this talk, but the slides are available below. These were used for the talk about our work on query optimization for dynamic imputation.","tags":null,"title":"VLDB: Query Optimization for Dynamic Imputation","type":"talk"},{"authors":null,"categories":null,"content":"","date":1474329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1474329600,"objectID":"faf4d254646fe9acb9cded331bf09584","permalink":"https://josecambronero.com/talk/kx-meetup-2016/","publishdate":"2016-09-20T00:00:00Z","relpermalink":"/talk/kx-meetup-2016/","section":"talk","summary":"In this talk, Dennis Shasha and I present some of our work on AQuery, an optimizing compiler for order-related queries. I also provide a tutorial on implementing your own domain-specific language on top of kdb+/q","tags":null,"title":"Kdb+ User Meetup September 2016","type":"talk"}]